First of all, I assume there is a DNS record in front of the load balancer which points to the web server and the relevant port. We should first create the database instance in AWS, depending of the preference of the database, sync the live database to it simultaneously. Than, we should create the Kubernetes cluster(s), push the relevant docker images for both the frontend and the backend applications to the Amazon ECR, and deploy those images to the relevant Kubernetes components, such as deployments, or pod's. Therefore, right now, we should have our backend, frontend and database instances ready, there should be the dataset of the live environment in the database and all those components should be connecting each other properly, as wished and designed.   

We should benefit from hpa, horizontal pod autoscaler component of Kubernetes to deal with high peak loads. I personally like to compare the cumulative CPU usage of the cluster to the predefined value, but of course there are other metrics to pay attention into as well. But, the autoscaling is personally the best reason to use Kubernetes cluster(s) for me and it's the relevant solution to take care of high peak loads. 

In order to create sandbox, or testing environments for the relevant team, in an another namespace of Kubernetes, we can create the identical environments with much less available resource and with autoscaling off, and with minimum amount of replicas. This can be created through a Jenkins pipeline, or some CI tool of the preference. There are numerous configuration management tools to run simple .yml files of related deployments and services on top of them and they can be easily get triggered by a Jenkins job, or part of a pipeline of preferred tool. The tester can just trigger a Jenkins job for instance called "Create a new sandbox environment", which can run several .yml files to create a Kubernetes namespace with the name of the user who triggered the relevant job, Kubernetes deployment of relevant backend and frontend services, a Kubernetes service with a ClusterIP, reachable via internet. In order to keep the environment clean and the costs under control, that Jenkins job can automatically trigger an another Jenkins job, let's say after a hour for instance, a period which should be enough for testing to be done, can inherit the name of the user who had triggered the previous job, and delete that Kubernetes namespace with that name.  

The deployment purposes can be handled by any configuration management and CI tool which will enable to rolling update the running docker image in the running Kubernetes components. We can deploy your code to bitbucket, or git, a Jenkins job can be triggered to trigger an Ansible playbook, which changes the tag of git repo in our Dockerfile, which creates the new docker images, which pushes them to Amazon ECR and which rolling updates the docker image of running Kubernetes components. There can be used numerous other tools and pipelines solutions, but this is just an illustration of a simple one. 

It's very easy to make blue/green deployment as well, let's just create two deployment .yaml's and name them as we wish, and a service .yaml as well. We must pay attention to make sure that in our deployment .yaml's, everything to be exactly the same except the field of metadata/labels/version. Our service .yaml should have that same value of  metadata/labels/version field in the "blue" deployment .yaml in the spec/selector/version field of itself, whenever we want to switch to your "green" deployment, let's just modify the field of spec/selector/version with the value of metadata/labels/version field of our "green" deployment .yaml. In order to automate this process, we just need to observe the status of "green" deployment, which can be obtained from .status.conditions field and than, we can use kubectl patch svc command to modify the "spec" of the service .yaml.   

We should expose the frontend services with ingresses and we would place a HTTPS Load Balancer in front of those those ingresses. That would help to create a better architecture for high peak loads as well and which will create the proper endpoint for the internet users to reach. 

So, at this current stage, we have our frontend and backend services up and running, we have the database instance(s) which simultaneously receives data from the bare metal solution and we have an exposed IP address. All we need is to change the DNS record in order the domain name to point the new IP address to complete the moving process from the datacenter to the AWS. It hadn't been specified in the question and it's anyway not logical to shut down the already up and running data center before moving and being sure everything is OK, therefore here goes the rollback scenario as well. If it's experienced that everything is not OK in the new setup, than we can just change the DNS record, be sure that database is up to date and we are back to the previous setup. 

In order to monitor the Kubernetes cluster, one of the most prominent tools to use will be Prometheus. The Helm chart of [kube-prometheus](https://github.com/coreos/prometheus-operator/tree/master/helm/kube-prometheus) will be enough to up&run Prometheus and Grafana as well. We can override values.yaml file according to your needs, such as setting login credentials, granting persistent storages, exposing Grafana as you wish, etc. 

For centralized logging purposes, we should stick with AWS' Elasticsearch service. All we need is a IAM user with "Programmatic access" permission and to modify the policy of Elasticsearch service to allow access of the user created, the NAT gateways of Kubernetes cluster. In order to forward logs of our Kubernetes cluster to the AWS' Elasticsearch, we can create a DaemonSet in our cluster by using a fluentd docker image which fulfills our needs, [this](https://hub.docker.com/r/cheungpat/fluentd-elasticsearch-aws/) is a relevant example for that, all we need is to override the required fields in order to authenticate your AWS environment, as well as the Elasticsearch service. 
